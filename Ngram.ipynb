{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Ngram model을 이용한 문장 자동 생성기**\n",
        "목표 : Ngram 모델을 이용하여 문장을 자동으로 생성하는 모델을 만들어보자.\n",
        "\n",
        "계기 : 자연어처리 시간에 배운 Ngram 모델이 흥미로워서 실제로 사용해보고 싶었고, 인터넷에 있는 좋은 참고자료들이 많아, 이를 읽고 ChatGPT를 활용하여 한국어 문장 자동 생성기를 만들게 되었습니다.\n",
        "\n",
        "---\n",
        "\n",
        "※ 본 프로젝트는 아래의 코드를 참고하여 구현되었으며, 일부 구조를 유지하고 있습니다.\n",
        "\n",
        "※ 어떠한 상업적인 목적 없이, 순수한 학습 목적의 개인 프로젝트임을 밝힙니다.\n",
        "\n",
        "※ Smoothing 적용, Perplexity 개선 및 한국어 처리를 중심으로 코드를 구성하였습니다.\n",
        "\n",
        "참고 자료 :\n",
        "- Joshua Loehr님의 N-gram Language Model 구현 :\n",
        "https://github.com/joshualoehr/ngram-language-model/blob/master/language_model.py\n",
        "- 고려대학교 NLP & AI Lab : https://github.com/nlpai-lab/nlp-bible-code/blob/master/09%EC%9E%A5_%EC%96%B8%EC%96%B4%20%EB%AA%A8%EB%8D%B8/%5B9-1%5D_N_gram_%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8%EB%A1%9C_%EB%AC%B8%EC%9E%A5_%EC%83%9D%EC%84%B1%ED%95%98%EA%B8%B0.ipynb\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qqVyr7HfyF5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ngram 모델이란?**\n",
        "- N-gram 언어 모델은 앞의 N-1개의 단어가 주어졌을 때, 다음 단어가 나올 확률을 기반으로 문장을 생성하거나 평가합니다.\n",
        "\n",
        "- n이 커질수록 문맥을 더 많이 반영할 수 있지만, 그만큼 학습 데이터에서 등장하지 않은 조합이 많아지게 됩니다.\n",
        "\n",
        "- 이처럼 학습데이터에 없는 n-gram으로 인해 등장 확률이 0이 되는 문제를 방지하기 위해 **smoothing**을 적용합니다.\n",
        "  - Smoothing이란?\n",
        "    - 학습 데이터에 없는 경우에도 확률이 0이 되지 않도록 아주 작은 값을 더해주는 방법입니다.\n",
        "\n",
        "    - Laplace smoothing, KneserNey smoothing, KatzBackoff smoothing 등이 있습니다.\n",
        "\n",
        "  - Laplace Smoothing (Add-1)\n",
        "    - 가장 기본적인 방법으로, 모든 조합 등장 횟수에 1을 더하는 방식입니다.\n",
        "    - 실제 count가 0이더라도 1회 등장한 것처럼 더합니다.\n",
        "    - 꼭 1이 아니라, 0.01, 0.05 와 같은 작은 값을 더할 수도 있는데, 이를 Add-λ Smoothing이라고 합니다.\n",
        "\n",
        "\n",
        "- nltk 라이브러리를 활용하여 N-gram 생성을 간편하게 처리할 수 있습니다."
      ],
      "metadata": {
        "id": "4F-Qzoc5zRls"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "dyV_ECQXGwB8"
      },
      "outputs": [],
      "source": [
        "## 실습 환경 세팅\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "import numpy as np\n",
        "import codecs\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 한국어 처리를 위한 사전작업\n",
        "!pip3 install JPype1-py3\n",
        "!pip3 install konlpy\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# nltk를 사용을 위하여 선행 패키지를 설치한다.\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vp-bQJhUHHFn",
        "outputId": "f08ee5d7-f90c-4a85-d40d-bd5eb31936aa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting JPype1-py3\n",
            "  Downloading JPype1-py3-0.5.5.4.tar.gz (88 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/88.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: JPype1-py3\n",
            "  Building wheel for JPype1-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for JPype1-py3: filename=JPype1_py3-0.5.5.4-cp311-cp311-linux_x86_64.whl size=3259382 sha256=e5766e1532d1f0a4d6ca384efddef1dba1747045ac5676f6495554ef2cffad10\n",
            "  Stored in directory: /root/.cache/pip/wheels/c3/a6/b5/d0acc5a6e1622b48518a0ac7266a98778336a0621b532e8f06\n",
            "Successfully built JPype1-py3\n",
            "Installing collected packages: JPype1-py3\n",
            "Successfully installed JPype1-py3-0.5.5.4\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.3.1)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.11/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy) (24.2)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m125.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.1/494.1 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.2 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 0. 데이터 다운로드\n",
        "# 문장 생성을위하여 네이버 영화 리뷰 데이터셋을 다운로드한다.\n",
        "%%time\n",
        "!wget -nc -q https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1R7gCB_Ze-d",
        "outputId": "33631e4c-d532-4653-ef40-59f0df0ff116"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.43 ms, sys: 18 µs, total: 3.45 ms\n",
            "Wall time: 103 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 1. 데이터로딩\n",
        "# 다운로드 받은 데이터셋에서 텍스트 부분만 가져온다.\n",
        "# codecs 패키지는 대용량 파일을 조금씩 읽을 수 있게 해준다.\n",
        "def load_korean_data(path):\n",
        "  with codecs.open(path, encoding=\"utf-8\") as f:\n",
        "    data = [line.split('\\t') for line in f.read().splitlines()] # \\n 제외\n",
        "    data = data[1:] # header 제외\n",
        "    docs = [row[1] for row in data]\n",
        "\n",
        "    return docs # text 데이터"
      ],
      "metadata": {
        "id": "kw3UFPD0M0KV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 2. 토크나이징\n",
        "tagger = Okt()\n",
        "def tokenize(text):\n",
        "  tokens = ['<s>'] + ['/'.join(t) for t in tagger.pos(text)] + ['</s>']\n",
        "  return tokens\n",
        "\n",
        "def preprocess(sentences, n):\n",
        "    tokens = []\n",
        "    for sentence in sentences:\n",
        "        toks = tokenize(sentence)  # 먼저 형태소 분석\n",
        "        tokens.extend(['<s>'] * (n - 1) + toks + ['</s>'])  # 문장 단위로 padding\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "YRMZZh2pNiu2"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 3. Language Model 클래스\n",
        "class LanguageModel:\n",
        "  def __init__(self, train_data, n=2, laplace=1.0):\n",
        "    self.n = n # 차수\n",
        "    self.laplace = laplace\n",
        "    self.tokens = preprocess(train_data, n)\n",
        "    self.vocab = set(self.tokens)\n",
        "    self.vocab_size = len(self.vocab)\n",
        "    self.model = self._create_model()\n",
        "\n",
        "  def _create_model(self):\n",
        "    if self.n == 1:             # 만약 unigram이라면\n",
        "      total = len(self.tokens)  # 전체 단어 개수에서\n",
        "      freq = FreqDist(self.tokens) # 단어 하나하나의 등장만 계산\n",
        "      return {(w, ): freq[w] / total for w in freq}\n",
        "\n",
        "    else:\n",
        "      return self._smooth()\n",
        "\n",
        "  def _smooth(self):\n",
        "    '''\n",
        "    토큰 시퀀스 = ['<s>', '재미/Noun', '있다/Adjective', </s>']\n",
        "    n_grams = ('재미/Noun', '있다/Adjective')\n",
        "    m_grams = ('재미/Noun')\n",
        "\n",
        "    Laplace smoothing:\n",
        "      p(있다 | 재미) = (count(재미, 있다) + L) / (count(재미) + L * V(고유개수))\n",
        "    '''\n",
        "    n_grams = list(ngrams(self.tokens, self.n))\n",
        "    m_grams = list(ngrams(self.tokens, self.n - 1)) # ngram에서 마지막 단어만 제외한 (n-1) gram을 생성합니다. (재미, 있다.) -> (재미)\n",
        "\n",
        "    n_vocab = FreqDist(n_grams)\n",
        "    m_vocab = FreqDist(m_grams)\n",
        "\n",
        "    def smoothed_prob(n_gram):\n",
        "      m_gram = n_gram[:-1] # 앞 부분\n",
        "      n_count = n_vocab[n_gram]\n",
        "      m_count = m_vocab[m_gram] # 앞 부분의 등장 횟수\n",
        "      return (n_count + self.laplace) / (m_count + self.laplace * self.vocab_size) # (전체 등장 횟수에 라플라스 더한 값) / (앞 부분의 등장횟수 + 라플라스 * 고유개수)\n",
        "\n",
        "    return {ng : smoothed_prob(ng) for ng in n_vocab} # 확률 분포로 만든다. ex: (있다. | 재미 )) : 0.0032\n",
        "\n",
        "  '''\n",
        "  Laplace와 다른 점이 무엇인가?\n",
        "  1. Laplace를 적용하여 해결되는 경우 :\n",
        "    학습에 (n-1) gram은 있었지만, 그에 이어지는 특정 단어가 없었던 경우\n",
        "\n",
        "    (예) (재미/Noun)와 (재미, 있다)는 있으나, (재미, 귀엽다)는 없는 경우 -> Laplace가 적용되어서 아주 작은 값이 나온다.\n",
        "\n",
        "  2. 아래의 함수를 적용하여 해결되는 경우 :\n",
        "   학습 데이터에 \"초콜릿\" 자체가 한 번도 안 나온 경우\n",
        "   그러면 n-1 자체가 없기 때문에 Laplace를 적용해도 불안정할 수 있다.\n",
        "\n",
        "  그러므로 oov 까지 적용하여 안정적인 모델을 만들고자 한다.\n",
        "  '''\n",
        "  def _convert_oov(self,ngram): # 단어 미등록 문제 해결 && 학습에 존재하는 일반적인 ngram일 경우에 그대로 반환\n",
        "    ngram = (ngram, ) if isinstance(ngram, str) else ngram # n =1 일 때 tuple로바꿔준다.\n",
        "    masks = list(reversed(list(product((0,1), repeat=self.n)))) # 어떤 위치의 단어를 <UNK>로 바꿀지 경우의 수 계산\n",
        "    for mask in masks:\n",
        "      alt = tuple(token if m else \"<UNK>\" for token, m in zip(ngram, mask)) # <UNK>로 토큰을 치환한 후에 모델에 존재하는 n-gram을 찾는다.\n",
        "      if alt in self.model:\n",
        "        return alt\n",
        "\n",
        "    return ngram\n",
        "\n",
        "  # 문장의 혼란도를 계산한다.\n",
        "  def perplexity(self, sentences):\n",
        "    '''\n",
        "    문제점 :\n",
        "    generate_sentence_from_seed에서 return을 self.perplexity([sentence])로 받았다.\n",
        "    그러나 이렇게 받으면 문제가 생길 수 있다.\n",
        "    문장을 생성한 후 다시 preprocess를 적용하면, 이미 형태소 분석이 된 token을 다시 분석하게 되어 형태가 깨질 수 있다.\n",
        "    예) '있다/Adjective' -> 다시 분석되며 '<s>', '있다', 'Adjective'처럼 잘못 나뉠 수 있음.\n",
        "\n",
        "    -> 모델에 존재하지 않은 n-gram이 생기게 되어 확률이 매우 작아지게 되고, Perplexity가 비정상적(48000 등)으로 커지는 문제가 발생하였다.\n",
        "    '''\n",
        "    test_tokens = preprocess(sentences, self.n)\n",
        "    test_ngrams = list(ngrams(test_tokens, self.n))\n",
        "    N = len(test_ngrams)\n",
        "\n",
        "    '''\n",
        "    가장 잘 나타내는 ngram tuple을 찾아 반환\n",
        "    -> self.model 딕셔너리에서 .get()을 통해 해당 n-gram의 확률을 가져온다.\n",
        "    -> perplexity를 로그 확률로 평균을 내어 계산한다.\n",
        "    -> 값이 낮을수록 더 자연스러운 문장이다.\n",
        "    '''\n",
        "\n",
        "    probs = [self.model.get(self._convert_oov(ng), 1e-8) for ng in test_ngrams] # self.model의 딕셔너리에서 .get(key, default)를 통해 확률을 불러온다.\n",
        "    return math.exp(-sum(map(math.log, probs)) /N)\n",
        "\n",
        "  # 개선 함수 : 이미 토큰화된 결과를 그대로 받아 Perplexity 계산\n",
        "  def perplexity_from_tokens(self, tokens):\n",
        "    # preprocess된 token 리스트를 그대로 입력받아 평가\n",
        "    test_ngrams = list(ngrams(tokens, self.n))\n",
        "    N = len(test_ngrams)\n",
        "    probs = [self.model.get(self._convert_oov(ng), 1e-8) for ng in test_ngrams]\n",
        "    return math.exp(-sum(map(math.log, probs)) / N)\n",
        "\n",
        "\n",
        "  def generate_sentence_from_seed(self, seed=None, min_len = 10, max_len = 30, debug = False):\n",
        "    if seed is not None:\n",
        "      random.seed(seed)\n",
        "\n",
        "    # 언어 모델에서 가장 확률이 높은 n-gram 조합을 이용하여 한 단어씩 생성한다.\n",
        "    sent = [\"<s>\"] * max(1, self.n - 1)# 문장 시작 시에 앞의 단어가 없으므로 <s> 토큰을 사용하여 문장의 시작을 표시한다.\n",
        "    while sent[-1] != \"</s>\":           # 매번 다음 단어를 예측해서 문장에 추가한다.\n",
        "      prev = tuple(sent[-(self.n - 1):]) if self.n > 1 else() # 현재 시점에서 이전 (n - 1) 개의 단어를 뽑는다.\n",
        "      candidates = [(ngram[-1], prob) for ngram, prob in self.model.items() if ngram[:-1] == prev] # 학습된 ngram 모델에서 ngram의 앞부분(n-1)이 prev와 같다면, 그 ngram의 마지막 단어를 candidates로 뽑는다.\n",
        "\n",
        "      if not candidates:\n",
        "        sent.append(\"</s>\")\n",
        "        break\n",
        "\n",
        "      tokens, probs = zip(*candidates) # 대표로 뽑은 단어와 그에 해당하는 확률을 뽑는다.\n",
        "      total = sum(probs)\n",
        "      probs = [p / total for p in probs] # 확률을 전체에서 정규화한다.\n",
        "      next_token = random.choices(tokens, weights = probs, k = 1)[0] # 확률에 따라서 뽑고 그것을 결과로 냅니다.\n",
        "\n",
        "      if debug:\n",
        "         print(f\"{prev} -> {next_token}\")\n",
        "\n",
        "      sent.append(next_token)\n",
        "      if len(sent) >= max_len:\n",
        "        sent.append(\"</s>\")\n",
        "\n",
        "    sentence = ' '.join(sent)\n",
        "    return sentence, self.perplexity_from_tokens(sent)\n",
        "\n",
        "  def clean(self, raw_sentences):\n",
        "    tokens = raw_sentences.split()\n",
        "    cleaned = []\n",
        "\n",
        "    for token in tokens:\n",
        "      if token in (\"<s>\", \"</s>\"):\n",
        "        continue\n",
        "      if \"/\" in token:\n",
        "        word = token.split(\"/\")[0]\n",
        "        cleaned.append(word)\n",
        "      else:\n",
        "        cleaned.append(token)\n",
        "\n",
        "    return \" \".join(cleaned)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "TVISVefxNycA"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 4. 실행 ###\n",
        "if __name__ == '__main__':\n",
        "    from itertools import product\n",
        "\n",
        "    # 하이퍼파라미터 설정\n",
        "    n = 3\n",
        "    laplace = 0.05\n",
        "    num_sentences = 10\n",
        "\n",
        "    # 데이터 불러오기\n",
        "    print(\"데이터 로딩 중...\")\n",
        "    train_sentences = load_korean_data(\"ratings_train.txt\")\n",
        "    print(f\"총 {len(train_sentences)}개의 문장 불러옴.\")\n",
        "\n",
        "    # 1만개만 선택\n",
        "    random.shuffle(train_sentences)\n",
        "    subset = train_sentences[:20000]\n",
        "\n",
        "    # 모델 학습\n",
        "    print(f\"{n}-gram 모델 학습 중 (Laplace smoothing = {laplace})...\")\n",
        "    lm = LanguageModel(subset, n=n, laplace=laplace)\n",
        "\n",
        "    # 문장 생성 및 평가\n",
        "    print(\"\\n생성된 문장들:\")\n",
        "    results = [lm.generate_sentence_from_seed(seed=i) for i in range(num_sentences)]\n",
        "    for i, (s, ppl) in enumerate(results):\n",
        "        cleaned = lm.clean(s)\n",
        "        print(f\"{i+1}. {cleaned} (Perplexity : {ppl : .3f})\")\n",
        "    # 가장 좋은 문장 선택\n",
        "    best = min(results, key=lambda x: x[1])\n",
        "    print(\"\\n가장 자연스러운 문장:\")\n",
        "    print(f\"{lm.clean(best[0])  } (Perplexity: {best[1]:.3f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-EjPzIjR-RZ",
        "outputId": "b5667c92-c8fb-4592-c0f6-927b32d2f457"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 로딩 중...\n",
            "총 150000개의 문장 불러옴.\n",
            "3-gram 모델 학습 중 (Laplace smoothing = 0.05)...\n",
            "\n",
            "생성된 문장들:\n",
            "1. 어메이징 스파이더맨 ! (Perplexity :  2730.786)\n",
            "2. 휴잭맨 과 아이유 의 유쾌한 일본 유랑 기 혹은 망고 나무 아래 (Perplexity :  1157.907)\n",
            "3. 봤더니 사랑 고 ㅏ 전쟁 ' 의 감독 이기도 하다니 .. (Perplexity :  1973.812)\n",
            "4. 너무 귀여운 돼지 베 이브 ~!!! (Perplexity :  504.655)\n",
            "5. 근거 없는 확신 이 지나고 나면 별일 아닌것 처럼 느껴지지만 그것 이 외면 할 수 없었음 !!! (Perplexity :  338.291)\n",
            "6. 저 학교 1 층 에는 창문 도 없나 (Perplexity :  1477.127)\n",
            "7. 20 살 까지의 인생 ' 그 이상 의 작품 ㅎㅎㅎ ^^ (Perplexity :  1809.119)\n",
            "8. 참 .. 재미없다 .. (Perplexity :  153.756)\n",
            "9. 평생토록 잊지못 할 .... (Perplexity :  901.703)\n",
            "10. 매트릭스 의 꿈 의 영상 실록 (Perplexity :  280.990)\n",
            "\n",
            "가장 자연스러운 문장:\n",
            "참 .. 재미없다 .. (Perplexity: 153.756)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **느낀 점**\n",
        "\n",
        "- 이번 프로젝트를 통해 N-gram 모델을 구현해보며,최신의 대규모 언어모델보다는\n",
        "\n",
        "  성능이 아쉽지만, 기초적인 언어 생성 모델의 작동 원리를 이해할 수 있었습니다.\n",
        "\n",
        "\n",
        "- 특히 **Smoothing 기법**을 활용해 볼 수 있어 좋았습니다.\n",
        "\n",
        "- 처음엔 한국어 문장분석이 어렵지 않을까 걱정했지만, 형태소 분석 라이브러리가 있어 수월하게 진행할 수 있었습니다.\n",
        "\n",
        "  언제나 문장의 시작점과 끝점은 꼭 명시해주는 것이중요하다는 사실도 알았습니다.\n",
        "\n",
        "- N-gram에서 이전 데이터들을 많이 포함할수록(n이 커질수록) 문장이 읽기가 좋아지는 편인 것 같습니다.\n",
        "\n",
        "- 마지막으로, 단순히 코드를 복사/붙여넣기 하지 않고,\n",
        "직접 주석을 달아보고, 코드를 이해하고, 오류를 개선하는 과정에서 많이 공부하였습니다.\n",
        "\n",
        "- 생각보다 시간이 오래걸렸지만, 의미 있었던 토이프로젝트라고 생각합니다."
      ],
      "metadata": {
        "id": "oCx6f1VYyDj4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "umYbPymANJuY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}